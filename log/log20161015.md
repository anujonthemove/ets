## Captain's Log: and so begins a library
*Oct 15, 2016*

### Fixes to notebook for Holt's linear method

Today started off well for this project. As an accidental warmup, I noticed that my parameterization of Holt's linear method was a bit different from Hyndman's, and correcting that discrepancy means the results now match the R results more closely. Yay!

The relevant bit of the diff is here if you're curious: [569ab5b#diff-f1fc279a952076f7e4354cf02ec8d299R87](https://github.com/mcskinner/ets/commit/569ab5b42554c0e98eec3fbd3cacf9eb5f220ed2#diff-f1fc279a952076f7e4354cf02ec8d299R87).

### Generalizing the models

Having resolved that quirk, and as promised last time, I turned my attention toward reusability. While my initial experiments turned out well, the code is embedded in Jupyter notebooks and, for simplicity, the model is parameterized with a bunch of variables rather than a more compact tensor-based representation.

The first part of that process, at least, was very easy. Just copy the code from Jupyter and then paste it into a new file: [55739cf](https://github.com/mcskinner/ets/commit/55739cfa4cbcd58f4e1dd3b4abf53c595a4bd303).

The second step... that was less painless. Although the resulting diff is actually pretty small ([e76e67c](https://github.com/mcskinner/ets/commit/e76e67cdcfc2c4e14afc3d70b1c2303027746a65)), I had to work for quite some time to get all of the tensors shaped correctly. In particular, `pack_out` was coming out as a 2d tensor (i.e. a matrix) rather than a 1d vector. That led to a mismatch with the shape of `data`, which led TensorFlow to broadcast the subtraction into a large 2d form, which then caused very large and not helpful cost values, which blew up the gradients and the optimization. It took me more looks than I'd care to admit before I realized that the initially reported costs of 6000 were several orders of magnitude past reasonable.

Fortunately, the third step was easy again. The advantage of the tensor-based representation is that, once it's working, you can easily change the models just by updating the measurement vector `w` and the transition matrix `F`. The last commit does just that: change `w` and `F` from constants to params, and then show how both simple exponential smoothing and Holt's linear method are built from that: [98d2be9](https://github.com/mcskinner/ets/commit/98d2be98dcb48571988bd5a29582b40599d241e8).

### And then came variable support

After a simple trend, the next step up in model complexity is to support damped trends. This requires a third tuning parameter, `phi`, indicating how much the trend should decay each step. The relevant equations:

`forecast[t+1] = level[t] + phi * pace[t]`<br>
`pace[t+1] = phi * pace[t] + beta * error[t+1]`

Clearly the most natural place for this tuning parameter to sit is inside the measurement and transition equations (in `w` and `F`, respectively). For example, we want a `w` of `[1, phi]'`.

So I added some primitive variable support to the modeling "library" by taking advantage of Python's loose variable typing. If you want a variable, send in a string name: [adcef0a](https://github.com/mcskinner/ets/commit/adcef0aec9f08020460749f726ea9738cc50d3ea).

Of course, when I tested that on my silly sample data, the optimizer did ... curious things. Because I don't constrain the parameters in any way, the best parameters approached an `alpha` above one (1.2), a negative `beta` (-0.05), and a `phi` that was also above one (1.09).

So that's weird. Probably I should add support for bounding the parameters if I want to match more traditional results. And certainly I should stop using such artificial data and instead use some more classic examples. Not that a `phi` above one is necessarily bad mind you, if you believe a trend can accelerate.

### Next steps

So now I've got a feel for how this will work as a library, but it's still pretty clearly a one-off script, and the model diagnostics are pretty weak. Next up:

* Pull the optimization incantations out into a helper method.
* Add flags for specifying the input data set and the model choice.
* Add AIC as a model diagnostic, one of Hyndman's favorites.
* Run the models on Hyndman's sample data from the [expsmooth package](https://cran.r-project.org/web/packages/expsmooth/expsmooth.pdf).
* Add support for bounding the parameter values.

