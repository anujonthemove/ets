## Captain's Log: seasonal models
*Oct 17, 2016*

So the headline says it all, I've added support for additive seasonality, and then used it to fit the `expsmooth::ukcars` dataset from R. So now that's 2 out of the 4 exercise done from Hyndman 2.3!

### Framework updates

Per usual, getting this working seemed as simple as just adding a new model definition with the right measurement and update equations, but actually took several small but important enhancements to the framework to get working completely. So what were they?

* Allow for constants in the parameter vector `g` (called `param_vars` in the code). You only update the active seasonal component each time, and the rest of those "params" need to be set to 0. In simpler cases this also just allows you to name the parameters in the vector however you like.
* Tidy the parameter initialization and boundary code to work for the starting parameters as well.
* Use TensorFlow's `AdamOptimizer` instead of the trickier gradient descent with clipping. Convergence is now much faster and more reliable.
* Add a pre-optimization for just the starting state, which is a cheap stand-in for a more proper variable initialization tactic. It works for now.

And then of course I had to actually add the `QuarterlySeasonal` model, but that's just a metadata definition on top of the framework.

Code is here: [587a47e](https://github.com/mcskinner/ets/commit/587a47e0959f421312f5ec5ce74b4e8490a89d16).

### Replicating results for Hyndman Exercise 2.3 (part deux)

So last time I matched the results for `expsmooth::bonds`, which was easier because that was only a damped trend. This time we face `expsmooth::ukcars`, which throws out the trend component, but adds a quarterly seasonal component.

So what did R say?

`> summary(ets(ukcars, 'ANA'))`

... (paraphrased) ...

`cost = 72089.76, state0 = [313.0916  -1.1271  -44.606  21.5553  24.1778], varz = {'alpha': 0.6267, 'beta': 1e-04}`

And what could this TensorFlow stack manage?

`Training cost= 71925.156250000 state0= [ 267.42416382   76.4876709    72.12495422    5.96319818   49.40699387] varz= {'alpha': 0.616916, 'gamma': 0.00013441686}`

Okay so `alpha` and `gamma` are close, but the starting state is not. There are two reasons for that. First, R constrains the seasons to add up to 0 and I didn't, because that would have been more work. Second, R lists the seasons backwards due to a mild parameterization difference.

So we just need to swizzle our numbers around a bit. Move the starting level up by 50, the starting seasons down by 50 to compensate, and then reverse the seasons. That leaves you with:

`Training cost= 71925.156250000 state0= [ 317.42416382   -0.59300613    -44.03780182   22.12495422    26.4876709] varz= {'alpha': 0.616916, 'gamma': 0.00013441686}`

And, indeed, that looks a fair bit like the result from R, albeit slightly better optimized per usual. Yay! But, per usual, this actually rquired a fair bit of tweaking to the meta-parameters in order to get a good solution. Using `AdamOptimizer` helped a lot, but there is still significant room for improvement.

### Playing with the unconstrained model

As is becoming something of a habit, I have to know what happens when you let the parameters wander outside of their typical range. For the `ukcars` timeseries, this happens:

`Training cost= 53595.265625000 state0= [ 266.70498657   79.45609283   80.24089813   -4.56693745   41.97756577] varz= {'alpha': 0.71806455, 'gamma': -0.21096832}`

Unsurprisingly the seasonal conditions stay about the same, but the `level` favors a higher `alpha` to incorporate new information faster, while the `gamma` implies that you should expect any new seasonal effects to reverse! Now, that seems kind of weird, but it does make a certain sort of sense. Especially in measures of production or consumption, you may expect that a big push this year may lead to a mild slump next year.

How does R compare? Well first, you have to coerce the `ets` method pretty hard to give you this same result, since it doesn't even believe that a negative `gamma` is admissible! But once you do that, you get:

`> summary(ets(ukcars, 'ANA', bounds = 'usual', lower = c(0, 0, -1, 0)))`

... (paraphrased) ...

`cost = 52181.34, state0 = [349.9355  -4.6355  -55.8698  34.4432  26.0622], varz = {'alpha': 0.9619, 'gamma': -0.259}`

Which actually kicks the pants off our result. As it turns out, the R numerical optimizer is actually pretty good. TensorFlow was having some trouble converging.

Back to tuning the metaparameters...
