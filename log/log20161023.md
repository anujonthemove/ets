## Captain's Log: Holt-Winters and beyond
*Oct 23, 2016*

Good news! Adding Holt-Winters as a forecasting technique did not require any real framework updates. I did fix up a long-standing annoyance with the debugging code, which no longer assumes a certain model size.

Code here: [8eb5cf8](https://github.com/mcskinner/ets/commit/8eb5cf8cdae20b406b4a098f204ab576c7e2114d).

### Taking it for a spin

So first things first, let's test the model out on the `ukcars` dataset from last time:

`Training cost= 71731.960937500 state0= [ 263.51660156    0.79273242   79.21785736   74.86268616    8.69106483   52.12995148] varz= {'alpha': 0.61260402, 'beta': 2.4115377e-06, 'gamma': 3.2716298e-06}`

As it turns out, the new parameters are of minimal use on this dataset. For reference, the simple seasonal model scored an SSE of `71925` or so. So this is only good for a gain of `200` or so, which is not nearly enough to justify the 2 extra parameters.

This is not surprising, since the `forecast::ets` code from R doesn't think those extra parameters are worth it either:

```
> ets(ukcars)
ETS(A,N,A) 
...
```

Which is to say, it prefers a model with *N*one for the trend parameter (vs *A*dditive or *M*ultiplicative).

### Another model variant

Okay, so the typical linear trend didn't add any value. But what if we try a not so typical use for those extra parameters? I have been thinking a lot about regression to the mean recently, and it seems like that might not be a bad idea for these timeseries models. Maintaining a steady baseline state for that purpose might allow the normal `level` parameter to be a bit more aggressive in chasing short-term 

This requires two small changes to the typical Holt-Winters model. First, we stop using the `pace` component when forecasting and instead reserve it for updating the `level` component, because that's how regression to the mean works. Second, we add a new parameter `reg` to control the weight of the regression to the mean. To keep the interpretation simple, I have restricted that parameter to stay negative.

So what happens?

`Training cost= 62857.042968750 state0= [ 294.04516602    3.10395288   35.8242836    31.35396004  -34.8841629    8.58514118] varz= {'alpha': 0.46758103, 'beta': 0.10518733, 'reg': -0.52504766, 'gamma': 9.9387053e-06}`

Well, pretty much what I had hoped: the regression component stays substantially negative and the `steady` component changes more slowly than the normal 'level' component (i.e. `beta < alpha`). And also this leads to a substantial reduction in SSE, which according to AIC is enough to warrant...

`-113 * log(71925) / 2 -> -631.8609241457159`<br>
`-113 * log(62857) / 2 -> -624.246893502174`

...at least 3 new parameters, but not quite 4 new parameters. And the new model has 3 new parameters: `beta`, `reg`, and the initial state for `steady`.

So yay! This model is worth it!

### A note on model training

So training these guys is turning out to be a real bugger. Getting into a good initial state has proven increasingly difficult, and training without one is difficult because the update parameters are typically on the scale of `[0, 1]` while the initial parameters are on the scale of the data.

In the case of the `ukcars` data, that is on the order of hundreds for the initial level and tens for the seasonal components. This makes it hard to pick an effective learning rate, and probably hints that input data should be transformed by an effective scalar before going into this machinery. It probably also means that I need to get substantially better at picking good initial states.
