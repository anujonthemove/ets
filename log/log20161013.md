# Captain's Log: hello, world
*Oct 13, 2016*

So yesterday I finally sat down with the Hyndman forecasting book and started working on some TensorFlow models for the most basic of exponential smoothing techniques. It was a tossup as to whether or not the idea had any merit, but a couple of late night experiments yesterday proved out the idea enough to warrant a deeper dive. Here we go.

### Simple Exponential Smoothing

As one does, I started with the simplest form:<br>
`level[t+1] = level[t] + alpha * (y[t+1] - level[t])`<br>
`forecast[t+h] = level[t]`

Or:<br>
`forecast[t+1] = level[t]`<br>
`error[t+1] = y[t+1] - forecast[t+1]`<br>
`level[t+1] = level[t] + alpha * error[t+1]`

That is to say, the model only maintains a single piece of state (`level`), and it forecasts that value for all eternity. But when presented with new information, it incorporates some part of that error into the new state. That's the goal of the parameter `alpha`, to "filter" the information let into the state.

But how does one decide on a good `alpha`? Well, a reasonable start would be to find one that minimizes the sum of squared errors (SSE). While we're at it, let's also choose a `level[0]` to start from.

The first notebook does just that, using TensorFlow to build out the model in very childish and verbose detail. I hope that makes it easier to understand, because it's all I could manage to understand myself:

https://github.com/mcskinner/ets/blob/master/intro/SimpleETS.ipynb

### Holt's Linear Method

Next up comes the trend component, and the model grows slightly more complex:<br>
`forecast[t+1] = level[t] + pace[t]`<br>
`error[t+1] = y[t+1] - forecast[t+1]` (same as before)<br>
`level[t+1] = level[t] + alpha * error[t+1]` (same as before)<br>
`pace[t+1] = pace[t] + beta * ((level[t+1] - level[t]) - pace[t])`<br>

As you can see, now we have a `pace` component to the state, which dictates how much we think the `level` will change every time step. This affects the forecast, and we also need a new update step for the new state, but otherwise we're in a pretty similar boat to before.

So, that leads to the second notebook, which is a similar transliteration of these thoughts into a vaguely Pythonic form:

https://github.com/mcskinner/ets/blob/master/intro/SimpleWithTrendETS.ipynb

And that's where I got in my evening. Both of the models matched up pretty well with Hyndman's `forecast` package in R, so I think that is a good sign. The model with trend did come out slightly different for the test data, but it might be that the numerical optimization in R is not as good.

### Next Steps

So I have really just scratched the surface here. The notebooks are hacky, TensorFlow is barely being stretched, and the ETS techniques here are very basic. So that's basically the roadmap:

* Move the TensorFlow model fits into proper Python classes, add tests, etc.
* Generalize the state space update using matrices instead of custom arithmetic.
* Build out seasonal models (aka Holt-Winters, aka Triple Exponential Smoothing).
* Start using TensorFlow's power for some fun non-traditional analyses.
