## Captain's Log: heuristic initialization
*Nov 6, 2016*

Last night I managed to get Nelder-Mead-in-TensorFlow to run successfully, and then to run quickly. However, I did not succeed in making it run effectively. Today, that changes.

### Revisiting last night's results

For a quick reference, here is the best that the optimizer could do with an initial state of all 0:

`Nelder-Mead cost= 1.546247602 state0= [ 0.2065111   0.01141049 -0.11022045  0.11017334 -0.24665834 -0.04433625] varz= {'alpha': array(0.7807469367980957, dtype=float32), 'beta': array(1.99460927774453e-08, dtype=float32), 'gamma': array(0.2503015398979187, dtype=float32)}`

If you don't scale the data first, it gets even worse:

`Nelder-Mead cost= 211565.437500000 state0= [ 0.00433559 -0.01714621  0.00827308  0.01068304 -0.0528009   0.00046154] varz= {'alpha': array(0.8017350435256958, dtype=float32), 'beta': array(0.00017842127999756485, dtype=float32), 'gamma': array(0.1494894027709961, dtype=float32)}`

The initial state never gets far away from 0, and previous results have shown that it should.

### Initializing the start state

I chose to implement the simple heuristic method outlined by Hyndman in section 2.6.1 of Forecasting with Exponential Smoothing. It works as follows:

* **Estimate the trend:** run a 2xM simple moving average (SMA) over the input time series, assuming even seasonality M (e.g. 4 for quarterly).<sup>1</sup>
* **Estimate the seasonal offsets:** subtract the trend from the raw data to obtain raw seasonal offsets, average those offsets for each seasonal index, and then normalize those averages to sum to 0.
* **Estimate the initial level and pace:** deseason the input series by subtracting the seasonal offsets, fit a linear regression to the first 10 deseasoned values, and use the intercept from that regression for the initial level and the slope for the initial pace.

I'm not entirely sure why "10" is the magic number here, but it seems to work pretty well. Probably something about how fast `alpha` and `beta` are expected to discount old information.

Code is here: [245f0b0](https://github.com/mcskinner/ets/commit/245f0b0bc5b79153d093836758325ffe0e9d4abd).

<sup>1</sup>The 2xM notation means first run a SMA with a window of M, then a second SMA with a window of 2. The reason for the second SMA is to keep the averages symmetric over the observations. Each moving average over a window `W` will shift the timeseries indices by `(W - 1) / 2`. For even W, that means the indices will be on half-integers, and the second `W = 2` SMA is there to shift them back onto proper integers. For more on moving averages, there is a good section in [Forecasting: principles and practice](https://www.otexts.org/fpp), by Hyndman and Athana­sopou­los: [Time series decomposition > 6.2 Moving averages](https://www.otexts.org/fpp/6/2).

### And the result?

So was it worth that mild effort to implement the initialization logic?

`Nelder-Mead cost= 72430.773437500 state0= [ 324.89303589    0.64923537   23.46473122   19.71313286  -47.09488297   -3.69610071] varz= {'alpha': array(0.6047322750091553, dtype=float32), 'beta': array(3.7764987581567766e-08, dtype=float32), 'gamma': array(0.009873298928141594, dtype=float32)}`

Definitely. That's very close to what R gets us, and this without any scaling of the input series. Of course, everything continues to work just fine on the scaled series:

`Nelder-Mead cost= 0.649365783 state0= [ 0.96064377  0.00231869  0.0695153   0.05697862 -0.13987188 -0.01249165] varz= {'alpha': array(0.6027776598930359, dtype=float32), 'beta': array(2.53897468383002e-07, dtype=float32), 'gamma': array(0.007053708657622337, dtype=float32)}`

Initialization sure does go a long way.

### Gradient descent

For fun I dusted off the gradient descent optimizer to see how it did with the initial state:

`Training cost= 0.645096660 state0= [ 0.95469576  0.00237715  0.07305736  0.06000036 -0.13842662 -0.00817003] varz= {'alpha': 0.61268902, 'beta': 4.8241436e-05, 'gamma': 3.4550685e-05}`

Pretty solid, and as usual a bit better than the heuristic optimization, but at the cost of much more time and tuning. And for the unscaled series:

`Training cost= 73830.054687500 state0= [ 315.23913574   -4.37847233   25.66602325   20.85948372  -45.01716232   -1.53468406] varz= {'alpha': 0.61854357, 'beta': 0.018964719, 'gamma': -6.6562166e-07}`

Also solid, but not as good as the faster Nelder-Mead optimization. Gradient descent is still a bit of a baby when the variable scales aren't close to uniform.

But still, initialization has proven to be well worth it across multiple optimization techniques.