## Captain's Log: late night experiments
*Oct 16, 2016*

So much to report...

### Replicating results for Hyndman Exercise 2.3

I lied. I only deserve partial credit here. I've only replicated the results for one of the four timeseries requested in the problem. But, to be fair, I did have to do it with TensorFlow instead of just using R. After much work, and many enhancements, I got there. Behold, the TensorFlow fit for `expsmooth::bonds`:

`Training cost= 7.064789772 state0= [ 5.31725359  0.64415807] params= [ 0.99999988  0.11887079] varz= {'phi': 0.80000031}`

This, compared to the R code, which I might note only turns in a paltry 7.075541 SSE, compared to the impressive 7.06479 displayed above:

`> ets(bonds, 'AAN', damped = TRUE, opt.crit = 'mse')`

... (paraphrased) ...

`state0 = [5.3338, 0.61], params = [0.9999, 0.1112], varz = {'phi': 0.8106}`

 Okay, so that is actually not _such_ an impressive gap. And clearly the actual results are not particularly different. Also, that 0.2% improvement to the error came with a significant performance hit. I imagine that implementing better intialization heuristics (as described in the book) would go a long way toward speeding convergence.

So anyway, what were the enhancements required to match up the results?

* Initial values for variables, at least besides the `alpha` and `beta` params that (for now) I have baked in elsewhere.
* Boundary conditions for those same variables, implemented via an escalating "soft cost" based on the L1 norm of how far out of bounds you are. This is weighted by a cost factor that is coupled to the global step.
* Speaking of which, finally implemented gradient clipping and the beginnings of a learning rate annealing schedule. This helped compress the required training epochs significantly.

And of course I updated the hard-coded timeseries in my code to reflect the data in `expsmooth::bonds`. That code is here: [8ba29ba](https://github.com/mcskinner/ets/commit/8ba29bac39bc26198c1b9a6b84245e535696076f).

### Loosening the bounds

I lied again. Or maybe this time I just misled. But, before I replicated the results above, I came up with numbers that were completely different. I actually forced R to come up with those same numbers to make sure I wasn't crazy.

The first results I got looked more like this:

`Training cost= 6.682221413 state0= [ 5.3789854   0.68180698] params= [ 1.35686934 -0.10649563] varz= {'phi': 0.76667231}`

And as it turns out, that's fine, but you need to tell R to loosen its collar as well:

`> ets(bonds, 'AAN', damped = TRUE, opt.crit = 'mse', bounds = 'admissible'))`

... (paraphrased) ...

`state0 = [5.3783, 0.6828], params = [1.3568, -0.1064], varz = {'phi': 0.7666}`

And that's for an SSE of 6.682221. So, indeed, it turns out I'm not that crazy after all. At least not here. It just turns out that you can in fact do better if you expect the level to increase by some growth rate, and if you expect today's trend to be reversed tomorrow. Assuming folks are reactionary is really not that unreasonable.

### Let phi drift

It turns out that, by default, the R `forecast` package restricts the trend decay parameter `phi` to `[0.8, 0.98]`. The upper end of that range seems fine, but it also seems not totally unreasonable to expect trends to decay much faster than that lower bound implies.

And, indeed, removing that bound when fitting the `expsmooth::bonds` series resulted in better results to the tune of 0.6%:

`Training cost= 7.022782803 state0= [ 5.17981148  0.97122931] params= [ 0.99990231  0.21476941] varz= {'phi': 0.66956896}`

So again, not a ton, but we've added up to about a whole 1% in ticky-tacky improvements by now. That's certainly something to sneer at.

### Decoupling the growth rates

Okay, great. By now I have some confidence that this TensorFlow stack is doing the same thing as the R packages. Time to see what happens if we try a few new models. For example, what if we try a different damping rate `phi` for the measurement vector `w` instead of the transition matrix `F`?

`Training cost= 6.841205597 state0= [ 3.07832909  2.80210972] params= [ 0.99998426  0.2892988 ] varz= {'phi_w': 0.99997699, 'phi_F': 0.56813312}`

This actually turns out to be a fairly interesting result. It is best to trust the most recent trend completely when forecasting, but to rapidly discount any accumlated trend in favor of new information. That model tweak is good for a 2.6% decrease in SSE versus the unconstrained `phi` model from above.

But is that worth the extra parameter? Akaike says...

`125 * log(7.022782803) -> 243.644943771`<br>
`125 * log(6.841205597) -> 240.370496622`

...yes! But barely.

We scored about a 3.3 improvement to the log-likelihood, and each variable costs 2 in the Akaike Information Criterion (AIC). So it is a win, and that means Hyndman's auto-forecasting framework would prefer this new two-phi formulation over the standard damped model.

### Decoupling ... well, everything

And now for something completely different. What happens if you let both the measurement vector `w` and the transition matrix `F` vary completely? Or at least, completely but with all parameters bounded to `[0, 1]`.

Well, this would happen:

`Training cost= 6.720711708 state0= [ 3.26171827  2.62790704] params= [ 0.47179505  0.80604774] varz= {'u': 0.9999994, 'v': 1.0, 'a': 0.88426256, 'b': 0.077508263, 'c': 0.4376882, 'd': 0.69070292}`

Where I've set up those variables so that:

`w = [u, v]'`<br>
`F = [[a, b], [c, d]]`

The most obvious result is that the measurement vector `w = [u, v] ~= [1, 1]`, even when freed from constraint, continues to assume the typical form of `level + pace`.

Meanwhile, the transition matrix is just weird. Having `pace` rely on the current `level` via a significant `c` term is ... yeah, weird. And the improvement in SSE only amounts to a 2.2 decrease to the log likelihood. Even if we throw out `u` and `v`, we've still added two variables but only improved the likelihood enough to warrant one of them.

### Convergence to common parameterizations

It's an interesting result above that the measurement equation stays intact in the simplest form across so many different paramterizations of this smoothing.

And it is worth noting that other runs of that same optimization do periodically end up in a state like this:

`Training cost= 6.782964230 state0= [ 2.84866428  3.00096464] params= [ 0.59967119  0.70373362] varz= {'u': 0.9988786, 'v': 0.99999881, 'a': 0.99442518, 'b': 0.29606944, 'c': 2.299983e-06, 'd': 0.79731995}`

Which suggests that maybe we should keep the measurement equation simple, but in the transition matrix we can let `level` and `pace` each damp the `pace` independently. So this is an alternative two-phi form, and indeed those results look pretty good:

`Training cost= 6.758851051 state0= [ 2.7786448  3.0975256] params= [ 0.99995416  0.28361905] varz= {'phi_b': 0.33941886, 'phi_d': 0.77824086}`

This looks pretty similar to the results above, but a bit better even because the optimizer had an easier time with fewer parameters. But then again, not quite as good as the weird form above.

### Chasing the dragon

And if you want to stay up until 3a taking it one step further, then what? Well you see how far you really can go with a fixed measurement equation:

`Training cost= 6.443121910 state0= [ 2.04471135  3.85026503] params= [ 0.99999982  0.21025231] varz= {'a': 0.82033187, 'b': 0.1861929, 'c': -4.6520512e-07, 'd': 0.99459034}`

As it turns out, you can go pretty far. That's good for an 8.3% improvement over the simple damped model, and almost 5% over the best two-phi form. And it seems to have happened with a supremely un-weird formulation. In addition to fixing the measurement vector to `[1, 1]'`, it also looks like you can fix the `pace` update to `[0, 1]'`, tantamount to maintaining your current estimate of pace completely.

### Model sensitivity

So of course, now we have to try that model directly too:

`Training cost= 6.682222843 state0= [ 2.08341599  3.81829381] params= [ 0.7706061   0.50461614] varz= {'a': 0.76667279, 'b': 0.23226939}`

And... that's not nearly as good. Maybe it's fine to force `c=0`, but actually important to keep that slight decay with `d=0.995`?

`Training cost= 6.435495853 state0= [ 1.90883005  3.96967173] params= [ 0.99999994  0.17311014] varz= {'a': 0.82258797, 'b': 0.16869266}`

Okay yes, that's more what we expected, and actually a bit better. What if we increase the decay a bit to `d=0.99` instead?

`Training cost= 6.594071388 state0= [ 2.00814605  3.89253283] params= [ 0.90291578  0.35834312] varz= {'a': 0.83073974, 'b': 0.19984148}`

That's not bad as bad as insisting on `d=1`, but that's also a pretty big performance difference. It's equivalent to a 3 point difference in AIC, which is enough to spend on `d` as a free parameter. Which is good news, because with that level of performance sensitivity it is safe to say that the selection of `d=0.995` was carefully optimized. it would be pretty cheap to claim that as a fixed constant in the model and not pay for it in AIC.

When given the choice, the optimizer wants it:

`Training cost= 6.433537960 state0= [ 1.89989316  3.97438025] params= [ 0.99999994  0.17539416] varz= {'a': 0.8236298, 'b': 0.16949651, 'd': 0.99469244}`

Even if you don't box the params into the `[0, 1]` range:

`Training cost= 6.293272018 state0= [ 1.89999998  3.97046161] params= [ 1.12975252  0.03583169] varz= {'a': 0.8265723, 'b': 0.162036, 'd': 0.99496549}`
