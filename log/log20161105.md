## Captain's Log: combatting instability
*Nov 5, 2016*

Stability continues to be a huge problem with training ETS models in TensorFlow. I'm beginning to suspect that it is not going to work well for the simple case, but perhaps some bootstrapping would do the trick.

### Resolving the state scale discrepancy

It is well documented that gradient descent methods struggle when parameter scales vary. One obvious improvement is to scale the input data down so it takes on a range similar to that of the other parameters. A simple division by the mean of the data does the trick quite nicely. I'm not quite sure why I was resistant to doing this before, opting instead for an interleaved state optimization, but it simplified things a fair bit.

Code is here: [88b5584](https://github.com/mcskinner/ets/commit/88b5584131dab7e390abf7ad63aef523c6f5d203)

### Nelder-Mead

Under the hood, Hyndman's `forecast` package is using the Nelder-Mead Simplex algorithm for parameter optimization. The immediate question, of course, is "can I use that myself?" The answer is mixed. I did have success coding up a cost function and plugging it into the `scipy` optimizer, but it still converges pretty slowly and erratically. That's likely due to my poor initialization logic.

Actually, after a bit of sleuthing, it turns out that I'm probably modifying the TensorFlow graph on every iteration, which leads to quadratic complexity. The later iterations are definitely much slower than the early ones.

The results look like so:

Nelder-Mead cost= 0.640850782 state0= [ 0.9803623  -0.19003025  0.07094056  0.05299543 -0.14185402 -0.01074522] varz= {'alpha': 0.62624007, 'beta': 0.652569, 'reg': -0.0092890728, 'gamma': 0.0016062602}

Note that `state0` is now scaled down by the mean of the data, roughly 333, and the cost is scaled down by the square of that. The best we'd seen before was closer to 0.55, so this result is only so-so.

Code is here: [1dc6ece](https://github.com/mcskinner/ets/commit/1dc6ece1a03bc66ad4cbf97a8e0ee6052a32b573)

### Aha! Placeholders!

So that's what the placeholders are for: operations you know you'll run every iteration in a repetitive way but with different data. Yep, Single Instruction Multiple Data (SIMD) rules the day with TensorFlow. So all I had to do was declare the slots for the variable assignments in advance, and then thread the new feeds all the way through the optimization and debugging diagnostics.

It's still pretty erratic, but at least now it runs very quickly once initialized. Of course, initialization is not particularly quick, and a full optimization still takes a bit over 10 seconds. Then again, it does boot up much faster than R...

Code is here: [30b431a](https://github.com/mcskinner/ets/commit/30b431aa173429b377fefff3b5111d14e6ba9492)

### Next steps

* Play with other `scipy` optimizers to see if they're better.
* Implement Hyndman's heuristic initialization procedures (simple moving average, etc).