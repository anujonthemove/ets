## Captain's Log: combatting instability
*Nov 5, 2016*

Stability continues to be a huge problem with training ETS models in TensorFlow. I'm beginning to suspect that it is not going to work well for the simple case, but perhaps some bootstrapping would do the trick.

### Resolving the state scale discrepancy

It is well documented that gradient descent methods struggle when parameter scales vary. One obvious improvement is to scale the input data down so it takes on a range similar to that of the other parameters. A simple division by the mean of the data does the trick quite nicely. I'm not quite sure why I was resistant to doing this before, opting instead for an interleaved state optimization, but it simplified things a fair bit.

Code is here: [88b5584](https://github.com/mcskinner/ets/commit/88b5584131dab7e390abf7ad63aef523c6f5d203)

### Nelder-Mead

Under the hood, Hyndman's `forecast` package is using the Nelder-Mead Simplex algorithm for parameter optimization. The immediate question, of course, is "can I use that myself?" The answer is mixed. I did have success coding up a cost function and plugging it into the `scipy` optimizer, but it still converges pretty slowly and erratically. That's likely due to my poor initialization logic.

Actually, after a bit of sleuthing, it turns out that I'm probably modifying the TensorFlow graph on every iteration, which leads to quadratic complexity. The later iterations are definitely much slower than the early ones.

The results look like so:

Nelder-Mead cost= 0.640850782 state0= [ 0.9803623  -0.19003025  0.07094056  0.05299543 -0.14185402 -0.01074522] varz= {'alpha': 0.62624007, 'beta': 0.652569, 'reg': -0.0092890728, 'gamma': 0.0016062602}

Note that `state0` is now scaled down by the mean of the data, roughly 333, and the cost is scaled down by the square of that. The best we'd seen before was closer to 0.55, so this result is only so-so.

### Next steps

* Figure out how to avoid so much expense from the TF graph.
* Play with other `scipy` optimizers to see if they're better.
* Implement Hyndman's heuristic initialization procedures (simple moving average, etc).